{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Chengyi Zhang's Site Hi, I am Chengyi Zhang, a UCB 2024 undergraduate. I am interesed in digital circuit design and verification, and is also actively learning about analog circuit design, computer architecture, and formal methods. My research interest is software hardware codesign and domain specific hardware accelerator, and I am currently an undergraduate researcher at the SLICE Lab @ Berkeley, working as a member of a robotics hw-sw-cosimulation project. I am also interested in computer-based education, and particularly about CS and hardware education. I am a member of the Algorithm & Computing for Education (ACE)Lab at Berkely, leading the redesign project of CS10, Project V. I am also exploring realms of CBE for hardware, and some early-stage works include Apollo, a logic complicator for the mass production of randmoized logic simplification exercises. This site keeps a record of some of my lecture notes (poorly documented) and some review notes (better documented), as well as some project reviews of what I find interesting and educative when doing them. Hope you find useful information here.","title":"Introduction"},{"location":"#chengyi-zhangs-site","text":"Hi, I am Chengyi Zhang, a UCB 2024 undergraduate. I am interesed in digital circuit design and verification, and is also actively learning about analog circuit design, computer architecture, and formal methods. My research interest is software hardware codesign and domain specific hardware accelerator, and I am currently an undergraduate researcher at the SLICE Lab @ Berkeley, working as a member of a robotics hw-sw-cosimulation project. I am also interested in computer-based education, and particularly about CS and hardware education. I am a member of the Algorithm & Computing for Education (ACE)Lab at Berkely, leading the redesign project of CS10, Project V. I am also exploring realms of CBE for hardware, and some early-stage works include Apollo, a logic complicator for the mass production of randmoized logic simplification exercises. This site keeps a record of some of my lecture notes (poorly documented) and some review notes (better documented), as well as some project reviews of what I find interesting and educative when doing them. Hope you find useful information here.","title":"Chengyi Zhang's Site"},{"location":"Untitled/","text":"","title":"Untitled"},{"location":"CS170/Asymptotics/","text":"Asymptotics Algorithm Well-defined procedure for carrying out some computational task. We want: Correct (Always halts + returns correct answer) & Efficient (Minimized consumption of computational resources like time, memory, etc.) Integer Arithmetic How is input represented? Indian-Arabic numeral system, representing numbers as sequence of digits in base 10 Addition Problem Given x,y in base 10, return x + y in base 10 Algorithm 1: repeated increments, start with x, and increment y times. Efficiency: increment takes less than or equal to n steps, and with y increments, => total number of stemps is less than or equal to 10^{n} \\cdot n , and can be simplified to 10^n steps considering carrying. Algorithm 2: adding up using a lookup table for each digit, with an efficiency of O(n) Cannot be better than Algorithm 2: Writing the answer down (n+1 digits) takes n steps, && Need to read at least all the input digits, exists inputs says the same answer for both. Multiplication Problem Given x,y in base 10, return x times y in base 10 Algorithm 1: repeated additions, start with 0, add x over and over for y times. Efficiency: \\underset {(n)} {num_{steps}} \\cdot \\underset {(y)} {num_{add}} = O(n \\cdot10^n ) Algorithm 2: grade school algorithm. Efficiency: O(n^2) Algorithm3: Divide and Conquer (Recursion) x = x_h \\cdot 10^{\\frac{n}{2}} + x_l y = y_h \\cdot 10^{\\frac{n}{2}} + y_l x \\cdot y = x_h \\cdot y_h \\cdot 10^{\\frac{n}{2}} + (x_h \\cdot yl + x_l \\cdot y_h) \\cdot 10^{\\frac{n}{2}}+ y_l \\cdot x_l Recurstion relation: T(n) \\leq 4*T(\\frac{n}{2}) + C\\cdot n if n >1, and T(n) \\leq C if n = 1, where C is an additional addition effort constant. Analysis : Total steps \\leq C \\cdot n \\cdot(1 + 2 + ... + 2^k) , where k = the height of the tree, logn, so the efficiency is \u00a5$O(n^2)$$ Algorithm 4: Karatruba's Algorithm x \\cdot y = \\underset{A}{x_h \\cdot y_h} \\cdot 10^{\\frac{n}{2}} + \\underset{C}{(x_h \\cdot yl + x_l \\cdot y_h)} \\cdot 10^{\\frac{n}{2}}+ \\underset{B}{y_l \\cdot x_l} Analysis : Reduced 4 recursive calls into only 3. Total steps \\leq C \\cdot n \\cdot(1 + \\frac{3}{2} + ... + \\frac{3}{2}^k) , where k = the height of the tree, logn, so < 3C \\cdot 3^{log_2n} \\lt O(n^{log_{2}3}) CS170 Lecture 2 Fibonacci Numbers Algorithm 1: Recursion def fib(n): if n <= 1: return n else: return fib(n-1) + fib(n-2) flops : calculate the # of \"flops\" -> the number of operation runtime : n * exp(n) In this model, there is a lot of redundant work. T(n) = 0 if n <= 1, else T(n) = T(n-1) + T(n-2) + 1 can prove T(n) == F(n+1) - 1, which is groing exponentially, making this algorithm very slow Algorithm 2: For Loop def fib(n): if n <= 1: return n else: a = 0 b = 1 for i in range (1, n-1): tmp = a + b a = b b = tmp return b flops : This is basically dynamic programming, taking n-1 flops. runtime : n^2 Algorithm 3: Fast Matrix Powering A = \\begin{bmatrix} 1 & 1\\\\ 1 & 0 \\end{bmatrix} A \\times \\begin{bmatrix} F_1\\\\ F_0 \\end{bmatrix} = \\begin{bmatrix} F_1+F_0\\\\ F_1 \\end{bmatrix} = \\begin{bmatrix} F_2\\\\ F_1 \\end{bmatrix} Need to multiply A for n times, then multiply the base case. By using the fast exponentiation algorithm, can reduce this to logn flops. runtime : n^2 * logn if we use the smart fast exponentiation, then n^2 if use a faster multiplication algorithm like karatsuba, can reduce it to n^1.5ish Algorithm 4: Closed Form Formula Diagonalize A as A = Q \\Lambda Q^T A^n = Q \\Lambda Q^T \\times Q \\Lambda Q^T ... \\times Q \\Lambda Q^T \\therefore A^n = Q \\Lambda^n Q^T \\therefore F_n = \\frac{1}{\\sqrt{5}} \\times [(\\frac{1 + \\sqrt{5}}{2}) ^ n - (\\frac{1 - \\sqrt{5}}{2}) ^ n] flops: logn as well Asymptotic Notation A way to compare the order of growth of functions. Big-O: less than or equal to Little-0: strictly less than Big-Omega: greater than or equal to Little-Omega: strictly greater than Theta: equal Ffn) = O(g(n)) \\implies \\exists \\ c> 0 \\ s.t. \\forall \\ n \\ f(n) \\le c \\times g(n) f(n) = o(g(n)) \\implies \\lim_{n\\to\\inf}\\frac{f(n)}{g(n)} = 0 f(n) = \\Omega(g(n)) \\implies g(n) = O(f(n)) f(n) = \\omega(g(n)) \\implies g(n) = o(f(n)) f(n) = \\theta(g(n)) \\implies g(n) = O(f(n)) \\ \\& \\ f(n) = \\Omega(g(n))","title":"Asymptotics"},{"location":"CS170/Asymptotics/#asymptotics","text":"","title":"Asymptotics"},{"location":"CS170/Asymptotics/#algorithm","text":"Well-defined procedure for carrying out some computational task. We want: Correct (Always halts + returns correct answer) & Efficient (Minimized consumption of computational resources like time, memory, etc.)","title":"Algorithm"},{"location":"CS170/Asymptotics/#integer-arithmetic","text":"How is input represented? Indian-Arabic numeral system, representing numbers as sequence of digits in base 10","title":"Integer Arithmetic"},{"location":"CS170/Asymptotics/#addition-problem","text":"Given x,y in base 10, return x + y in base 10 Algorithm 1: repeated increments, start with x, and increment y times. Efficiency: increment takes less than or equal to n steps, and with y increments, => total number of stemps is less than or equal to 10^{n} \\cdot n , and can be simplified to 10^n steps considering carrying. Algorithm 2: adding up using a lookup table for each digit, with an efficiency of O(n) Cannot be better than Algorithm 2: Writing the answer down (n+1 digits) takes n steps, && Need to read at least all the input digits, exists inputs says the same answer for both.","title":"Addition Problem"},{"location":"CS170/Asymptotics/#multiplication-problem","text":"Given x,y in base 10, return x times y in base 10 Algorithm 1: repeated additions, start with 0, add x over and over for y times. Efficiency: \\underset {(n)} {num_{steps}} \\cdot \\underset {(y)} {num_{add}} = O(n \\cdot10^n ) Algorithm 2: grade school algorithm. Efficiency: O(n^2) Algorithm3: Divide and Conquer (Recursion) x = x_h \\cdot 10^{\\frac{n}{2}} + x_l y = y_h \\cdot 10^{\\frac{n}{2}} + y_l x \\cdot y = x_h \\cdot y_h \\cdot 10^{\\frac{n}{2}} + (x_h \\cdot yl + x_l \\cdot y_h) \\cdot 10^{\\frac{n}{2}}+ y_l \\cdot x_l Recurstion relation: T(n) \\leq 4*T(\\frac{n}{2}) + C\\cdot n if n >1, and T(n) \\leq C if n = 1, where C is an additional addition effort constant. Analysis : Total steps \\leq C \\cdot n \\cdot(1 + 2 + ... + 2^k) , where k = the height of the tree, logn, so the efficiency is \u00a5$O(n^2)$$ Algorithm 4: Karatruba's Algorithm x \\cdot y = \\underset{A}{x_h \\cdot y_h} \\cdot 10^{\\frac{n}{2}} + \\underset{C}{(x_h \\cdot yl + x_l \\cdot y_h)} \\cdot 10^{\\frac{n}{2}}+ \\underset{B}{y_l \\cdot x_l} Analysis : Reduced 4 recursive calls into only 3. Total steps \\leq C \\cdot n \\cdot(1 + \\frac{3}{2} + ... + \\frac{3}{2}^k) , where k = the height of the tree, logn, so < 3C \\cdot 3^{log_2n} \\lt O(n^{log_{2}3})","title":"Multiplication Problem"},{"location":"CS170/Asymptotics/#cs170-lecture-2","text":"","title":"CS170 Lecture 2"},{"location":"CS170/Asymptotics/#fibonacci-numbers","text":"","title":"Fibonacci Numbers"},{"location":"CS170/Asymptotics/#algorithm-1-recursion","text":"def fib(n): if n <= 1: return n else: return fib(n-1) + fib(n-2) flops : calculate the # of \"flops\" -> the number of operation runtime : n * exp(n) In this model, there is a lot of redundant work. T(n) = 0 if n <= 1, else T(n) = T(n-1) + T(n-2) + 1 can prove T(n) == F(n+1) - 1, which is groing exponentially, making this algorithm very slow","title":"Algorithm 1: Recursion"},{"location":"CS170/Asymptotics/#algorithm-2-for-loop","text":"def fib(n): if n <= 1: return n else: a = 0 b = 1 for i in range (1, n-1): tmp = a + b a = b b = tmp return b flops : This is basically dynamic programming, taking n-1 flops. runtime : n^2","title":"Algorithm 2: For Loop"},{"location":"CS170/Asymptotics/#algorithm-3-fast-matrix-powering","text":"A = \\begin{bmatrix} 1 & 1\\\\ 1 & 0 \\end{bmatrix} A \\times \\begin{bmatrix} F_1\\\\ F_0 \\end{bmatrix} = \\begin{bmatrix} F_1+F_0\\\\ F_1 \\end{bmatrix} = \\begin{bmatrix} F_2\\\\ F_1 \\end{bmatrix} Need to multiply A for n times, then multiply the base case. By using the fast exponentiation algorithm, can reduce this to logn flops. runtime : n^2 * logn if we use the smart fast exponentiation, then n^2 if use a faster multiplication algorithm like karatsuba, can reduce it to n^1.5ish","title":"Algorithm 3: Fast Matrix Powering"},{"location":"CS170/Asymptotics/#algorithm-4-closed-form-formula","text":"Diagonalize A as A = Q \\Lambda Q^T A^n = Q \\Lambda Q^T \\times Q \\Lambda Q^T ... \\times Q \\Lambda Q^T \\therefore A^n = Q \\Lambda^n Q^T \\therefore F_n = \\frac{1}{\\sqrt{5}} \\times [(\\frac{1 + \\sqrt{5}}{2}) ^ n - (\\frac{1 - \\sqrt{5}}{2}) ^ n] flops: logn as well","title":"Algorithm 4: Closed Form Formula"},{"location":"CS170/Asymptotics/#asymptotic-notation","text":"A way to compare the order of growth of functions. Big-O: less than or equal to Little-0: strictly less than Big-Omega: greater than or equal to Little-Omega: strictly greater than Theta: equal Ffn) = O(g(n)) \\implies \\exists \\ c> 0 \\ s.t. \\forall \\ n \\ f(n) \\le c \\times g(n) f(n) = o(g(n)) \\implies \\lim_{n\\to\\inf}\\frac{f(n)}{g(n)} = 0 f(n) = \\Omega(g(n)) \\implies g(n) = O(f(n)) f(n) = \\omega(g(n)) \\implies g(n) = o(f(n)) f(n) = \\theta(g(n)) \\implies g(n) = O(f(n)) \\ \\& \\ f(n) = \\Omega(g(n))","title":"Asymptotic Notation"},{"location":"CS170/DP/","text":"Dynamic Programming","title":"DP"},{"location":"CS170/DP/#dynamic-programming","text":"","title":"Dynamic Programming"},{"location":"CS170/FFT/","text":"FFT Sorting Assume that no two numbers are equal => no need to check for equality => only less than computation * Depth of the tree is D * n! possible orderings of inputs * each possible input permutation needs a different output permutation to sort them the same * we have D >= log(n!) \\frac{n}{2}log(\\frac{n}{2}) \\leq log(n!) \\leq nlog(n) Selection Given A[1,...,n], return the kth smallest element in A. If using quick select, then it is a random algorithm on linear time Deterministic algorithm: Group the array A into groups of 5, calculate the median Then calculate the median recursively, then use it as the pivot in quick sort Then, partition the array into L, P, and R * if k < |L|, return select(L,k) * if k = |L|, return p * if k > |L|, return select(R, k-|L|-1) The median calculated this way is guaranteed to be greater and smaller than p > \\frac{n}{5}* \\frac{1}{2} * 3 = \\frac{3n}{10} T(n)\\leq T(\\frac{n}{5}) + T(\\frac{7n}{10}) + C*n It is linear and it can be proven by induction . Fast Fourier Transform Given two polynomials: A(x) = \\sum_{i=0}^{n-1}a_ix^i, B(x) = \\sum_{i=0}^{n-1}b_ix^i we want the coefficients of C(x) = (A*B)(x) c_k = \\sum_{i=0}^{n-1}a_ib_{k-i} Alg1: Straightforward Loop over the two loops k and i, resulting in O(n^2) complexity Alg2: Karatsuba A(x) = A_l + A_h * x ^{N/2}^ O(N^log2_3^) Alg3: Fast Fourier Transform if we know C evaluated at N distinct points, we can uniquely determine C X_i = \\omega^i, \\omega = e^{2\\pi i/2} F_{ij} = \\omega^{ij} FFT: given x, returns F*x a hat is FFT(a) b hat is FFT(b) for i = 0 to N-1, c_i hat gets a_i hat * b_i hat c = F^-1 * c hat A(x) = Aeven(x^2) + x * Aodd(x^2) T(N) = 2 * T(N/2) + O(N) Cross Correlation given vectors: x(x0, ... , xm-1) y(y0, ... , yn-1) we want: \\sum_{k=0}^{m-1}x_ky_{i+k} , where i = 0,1,...,n-m let X(z) = \\sum_{j=0}^{m-1}x_jz^j Y(z) = \\sum_{j=0}^{m-1}y_jz^j then, we can reverse X(z) to get them increment at the same tendency, arriving at: X(z) * Y(z) = (x_{m-1}y_0)z^0 + (x_{m-1}y_1 + x_{m-2}y_0)z^1 + ... + (x_0y_0+x_1y_1+ ... + x_{m-1}y_{m-1})z^{m-1} + (x_0y_1+x_1y_2+ ... )z^m + ... Reading off the coefficients will give the desired results.","title":"FFT"},{"location":"CS170/FFT/#fft","text":"","title":"FFT"},{"location":"CS170/FFT/#sorting","text":"Assume that no two numbers are equal => no need to check for equality => only less than computation * Depth of the tree is D * n! possible orderings of inputs * each possible input permutation needs a different output permutation to sort them the same * we have D >= log(n!) \\frac{n}{2}log(\\frac{n}{2}) \\leq log(n!) \\leq nlog(n)","title":"Sorting"},{"location":"CS170/FFT/#selection","text":"Given A[1,...,n], return the kth smallest element in A. If using quick select, then it is a random algorithm on linear time Deterministic algorithm: Group the array A into groups of 5, calculate the median Then calculate the median recursively, then use it as the pivot in quick sort Then, partition the array into L, P, and R * if k < |L|, return select(L,k) * if k = |L|, return p * if k > |L|, return select(R, k-|L|-1) The median calculated this way is guaranteed to be greater and smaller than p > \\frac{n}{5}* \\frac{1}{2} * 3 = \\frac{3n}{10} T(n)\\leq T(\\frac{n}{5}) + T(\\frac{7n}{10}) + C*n It is linear and it can be proven by induction .","title":"Selection"},{"location":"CS170/FFT/#fast-fourier-transform","text":"Given two polynomials: A(x) = \\sum_{i=0}^{n-1}a_ix^i, B(x) = \\sum_{i=0}^{n-1}b_ix^i we want the coefficients of C(x) = (A*B)(x) c_k = \\sum_{i=0}^{n-1}a_ib_{k-i}","title":"Fast Fourier Transform"},{"location":"CS170/FFT/#alg1-straightforward","text":"Loop over the two loops k and i, resulting in O(n^2) complexity","title":"Alg1: Straightforward"},{"location":"CS170/FFT/#alg2-karatsuba","text":"A(x) = A_l + A_h * x ^{N/2}^ O(N^log2_3^)","title":"Alg2: Karatsuba"},{"location":"CS170/FFT/#alg3-fast-fourier-transform","text":"if we know C evaluated at N distinct points, we can uniquely determine C X_i = \\omega^i, \\omega = e^{2\\pi i/2} F_{ij} = \\omega^{ij} FFT: given x, returns F*x a hat is FFT(a) b hat is FFT(b) for i = 0 to N-1, c_i hat gets a_i hat * b_i hat c = F^-1 * c hat A(x) = Aeven(x^2) + x * Aodd(x^2) T(N) = 2 * T(N/2) + O(N)","title":"Alg3: Fast Fourier Transform"},{"location":"CS170/FFT/#cross-correlation","text":"given vectors: x(x0, ... , xm-1) y(y0, ... , yn-1) we want: \\sum_{k=0}^{m-1}x_ky_{i+k} , where i = 0,1,...,n-m let X(z) = \\sum_{j=0}^{m-1}x_jz^j Y(z) = \\sum_{j=0}^{m-1}y_jz^j then, we can reverse X(z) to get them increment at the same tendency, arriving at: X(z) * Y(z) = (x_{m-1}y_0)z^0 + (x_{m-1}y_1 + x_{m-2}y_0)z^1 + ... + (x_0y_0+x_1y_1+ ... + x_{m-1}y_{m-1})z^{m-1} + (x_0y_1+x_1y_2+ ... )z^m + ... Reading off the coefficients will give the desired results.","title":"Cross Correlation"},{"location":"CS170/Graphs/","text":"Graphs Basics Convenient way of representing data, specifically obejcts and the pairwise relationship between those objects. Directed graphs: pair ( V (vertex), E (edges)) where E is a subset of ordered E -> E. If no self-loops exist, i.e some edge points to itself through a vertex, then the graph is simple . Undirected graphs: similar concept but, E is a subset of unordered E x E Weighted graphs: Every edge has an associated number E -> R N represents the number of vertices , and M represents the number of edges Cycles : where you can start from a vertex, traverse through the edges non repetitively, and go eventually go back to that vertex Strongly Connected Components : a subset of a directed graph in which all vertices can go to all other vertices. Every graph is a DAG of its SCCs. Application Road networks: weighted, directed Social media (Facebook): undirected (instagram/twitter/tiktok): directed Picture pixel: undirected Chat groups: (Hypergraphs), where a hyperedge is some subset of Vertices -> not just size 2! Implementation Adjacency matrix : 2D array, nxn booleans, where A[i][j] = 1 if and only if (i,j) is an edge Memory requirement: n^2 bits check adjacency: 1 loop through all neighbors: n Adjacency list : array of n pointer to linked lists, each contains the vertices that is an adjacent vertex (has an edge) Memory requirement: n + m bits check adjacency: degree of u + 1 loop through all neighbors: du + 1 Depth First Search: #global variables: clock: 1 visited[1,n] <- initially all 0 falses preorder[1,n], postorder[1,n] for all vertex u in vertex set: if not visited[u]: visited[u] = True explore(u) #The workhorse of this function! def explore(u): preorder[u] = clock++ for each (u,v) that belongs to Edges if not visited[v]: visited[v] = True explore(V) postorder[u] = clock++ Claim: DFS explore starting from v visits all vertices reachable from V. visited[u] = True if and only if V can reach U. DFS runtime: Adjacency matrix: O(n^2) Adjacency list: O(n + m) Classification of edges e = (u,v): 1. Tree edge: if e belongs to the DFS tree 2. Back edge: u is descendant of v not in DFS tree 3. Forward edge: u is an ancestor to a descendant and uv is not in the tree 4. Cross edge: All other edges for U -> V * if pre(U) < pre(V) < post(V) < post(U) => must be a tree edge or a forward edge * if pre(V) < pre(U) < post(U) < post (V) => must be a back edge * if pre(V) < post(V) < pre(U) < post(U) => must be a cross edge Claim : A Graph is a DAG if and only if DFS finds no back edges if back edge is found, then there exists a cycle if there's a cycle, start with the lowest preorder vertex in cycle V, do DFS, pre(V) < pre(U), so UV is a backedge as there exists a cycle DAG Topological Soring Given a DAG, sort vertices in order of their dependencies Theore: In a DAG, every pair of edge (U,V) has post post(V) < post(U) Algorithm: run DFS, then order form highest to lowest post order number Strongly Connected Components Connected : U and V are connected if path from u to v and from v to u if A and B are connected and B and C are connected, then A, B and C are all connected Parition all the vertices into disjoint sets where all the vertices in the set are connected to one another, each forming an SCC. => Every graph is a DAG of its SCCs. A Source SCC has no incoming edges from other SCCs, and a Sink SCC has no outgoing edges to another SCC. Fact: explore(V) stops when it has visted all vertices reachable from V Fact: if V belonging to a sink SCC, explore(V) only visits the sink SCC Algroithm: run explore on sink SCCs, and then remove all that is found. label = 0 repeat: if some V is not visited in a sink SCC: label++ explore V, give all vertice a label untill all vertices are labeled Identify sink SCC? if U has the highest post order #(visited last) => u is in a source SCC Form a Reversed graph Grev, and then find the source SCC in the reversed graph, it is actually a sink! Finding Shortest Paths Easiest: length of path from u to v is just the number of edges Algorithm: Breadth First Search (BFS) cost = O(m+n) Harder: Add weights to the edges, like distances/travel time/cost Algorithms: * if all the weights are postivie, use Dijkstra algorithm, at the cost of O((n+m)logn) if we use a binary heap * if weights can be negative(like in finance), must use Bellman-Ford, at the cost of O(mn) Naive Algorithm: initial begin pick source S, use Q(ueue), dist(s) = 0, for all v not is s, set dist(v) to infinity, and put S in queue while Q not empty: u = pop(Q) for all (U,V) where v is connected to u if dist(v) == infinity, ... not visited dist(u) = dist(V) + 1 add v to Q Social Network: Top Down: Run BFS for a few steps to find all the famous people Bottom up: for other vertices, ask if they are connected to any of the famous people. If found, can skip looking for a lot of other people","title":"Graphs"},{"location":"CS170/Graphs/#graphs","text":"","title":"Graphs"},{"location":"CS170/Graphs/#basics","text":"Convenient way of representing data, specifically obejcts and the pairwise relationship between those objects. Directed graphs: pair ( V (vertex), E (edges)) where E is a subset of ordered E -> E. If no self-loops exist, i.e some edge points to itself through a vertex, then the graph is simple . Undirected graphs: similar concept but, E is a subset of unordered E x E Weighted graphs: Every edge has an associated number E -> R N represents the number of vertices , and M represents the number of edges Cycles : where you can start from a vertex, traverse through the edges non repetitively, and go eventually go back to that vertex Strongly Connected Components : a subset of a directed graph in which all vertices can go to all other vertices. Every graph is a DAG of its SCCs.","title":"Basics"},{"location":"CS170/Graphs/#application","text":"Road networks: weighted, directed Social media (Facebook): undirected (instagram/twitter/tiktok): directed Picture pixel: undirected Chat groups: (Hypergraphs), where a hyperedge is some subset of Vertices -> not just size 2!","title":"Application"},{"location":"CS170/Graphs/#implementation","text":"Adjacency matrix : 2D array, nxn booleans, where A[i][j] = 1 if and only if (i,j) is an edge Memory requirement: n^2 bits check adjacency: 1 loop through all neighbors: n Adjacency list : array of n pointer to linked lists, each contains the vertices that is an adjacent vertex (has an edge) Memory requirement: n + m bits check adjacency: degree of u + 1 loop through all neighbors: du + 1","title":"Implementation"},{"location":"CS170/Graphs/#depth-first-search","text":"#global variables: clock: 1 visited[1,n] <- initially all 0 falses preorder[1,n], postorder[1,n] for all vertex u in vertex set: if not visited[u]: visited[u] = True explore(u) #The workhorse of this function! def explore(u): preorder[u] = clock++ for each (u,v) that belongs to Edges if not visited[v]: visited[v] = True explore(V) postorder[u] = clock++ Claim: DFS explore starting from v visits all vertices reachable from V. visited[u] = True if and only if V can reach U. DFS runtime: Adjacency matrix: O(n^2) Adjacency list: O(n + m) Classification of edges e = (u,v): 1. Tree edge: if e belongs to the DFS tree 2. Back edge: u is descendant of v not in DFS tree 3. Forward edge: u is an ancestor to a descendant and uv is not in the tree 4. Cross edge: All other edges for U -> V * if pre(U) < pre(V) < post(V) < post(U) => must be a tree edge or a forward edge * if pre(V) < pre(U) < post(U) < post (V) => must be a back edge * if pre(V) < post(V) < pre(U) < post(U) => must be a cross edge Claim : A Graph is a DAG if and only if DFS finds no back edges if back edge is found, then there exists a cycle if there's a cycle, start with the lowest preorder vertex in cycle V, do DFS, pre(V) < pre(U), so UV is a backedge as there exists a cycle","title":"Depth First Search:"},{"location":"CS170/Graphs/#dag-topological-soring","text":"Given a DAG, sort vertices in order of their dependencies Theore: In a DAG, every pair of edge (U,V) has post post(V) < post(U) Algorithm: run DFS, then order form highest to lowest post order number","title":"DAG Topological Soring"},{"location":"CS170/Graphs/#strongly-connected-components","text":"Connected : U and V are connected if path from u to v and from v to u if A and B are connected and B and C are connected, then A, B and C are all connected Parition all the vertices into disjoint sets where all the vertices in the set are connected to one another, each forming an SCC. => Every graph is a DAG of its SCCs. A Source SCC has no incoming edges from other SCCs, and a Sink SCC has no outgoing edges to another SCC. Fact: explore(V) stops when it has visted all vertices reachable from V Fact: if V belonging to a sink SCC, explore(V) only visits the sink SCC","title":"Strongly Connected Components"},{"location":"CS170/Graphs/#algroithm","text":"run explore on sink SCCs, and then remove all that is found. label = 0 repeat: if some V is not visited in a sink SCC: label++ explore V, give all vertice a label untill all vertices are labeled","title":"Algroithm:"},{"location":"CS170/Graphs/#identify-sink-scc","text":"if U has the highest post order #(visited last) => u is in a source SCC Form a Reversed graph Grev, and then find the source SCC in the reversed graph, it is actually a sink!","title":"Identify sink SCC?"},{"location":"CS170/Graphs/#finding-shortest-paths","text":"Easiest: length of path from u to v is just the number of edges Algorithm: Breadth First Search (BFS) cost = O(m+n) Harder: Add weights to the edges, like distances/travel time/cost Algorithms: * if all the weights are postivie, use Dijkstra algorithm, at the cost of O((n+m)logn) if we use a binary heap * if weights can be negative(like in finance), must use Bellman-Ford, at the cost of O(mn)","title":"Finding Shortest Paths"},{"location":"CS170/Graphs/#naive-algorithm","text":"initial begin pick source S, use Q(ueue), dist(s) = 0, for all v not is s, set dist(v) to infinity, and put S in queue while Q not empty: u = pop(Q) for all (U,V) where v is connected to u if dist(v) == infinity, ... not visited dist(u) = dist(V) + 1 add v to Q","title":"Naive Algorithm:"},{"location":"CS170/Graphs/#social-network","text":"Top Down: Run BFS for a few steps to find all the famous people Bottom up: for other vertices, ask if they are connected to any of the famous people. If found, can skip looking for a lot of other people","title":"Social Network:"},{"location":"CS170/Greedy/","text":"Greedy Algorithm Minimum Spanning Tree Given an undirected graph G, find the subset T of edges with the smallest total weight that connects all vertices. Fact : optimal solution must not have a cycle, because if a cycle exists, it is possible to remove an edge to maintain the connectivity and yet have a cheaper total weight Greedy Algorithm \uff1a repeat: pick the shortest path remaining that does not create a cycle until all is connected Tree is and undirected graph that is connected and has no cycle. (any vertex can be the root) Claim1 : Any 2 of these properties implies the third and implies that the graph is a tree 1. T is connected 2. T has no cycles 3. e = v - 1 Cut in Graph : is a partition of the vertices into some subsets and its complement, S and V - S, and possibly edges crossing it. Claim2 : lightest edge in any cut also appears in some MST. Claim3 : Suppose x has no edges connecting some cut, and we want to add an edge, suppose e is the lightest edge connecting the cuts, then X + e is a MST Meta-Algorithm\uff1a repeat: pick a cut such that x does not cross the cut, add the edge e with smallest weight edge in cut and add it to x until n-1 edges are added Kruskal : //Naive\uff1a x = empty, sort all edges by their weight for all edges in increasing order of weight, if x + e has no cycle, add it in x. //cost = cost of DFS per edge = O(m * n) //1st optimization: x = empty, sort all edge by their weight for all vertices, make set (v). for all e = (u,v) increasing order, if find(u) != find(v) which finds their connected components, then we merge u and v's connected components //cost = O(m x logn) => can get to O(m x log*n) Union-find Implementation : for each v in V, pi(v) = a pointer of the parent of v in a tree defining a connected component rank(v) = height of the subtree rooted at V def makeset(v): v.pi = v, v.rank = 0 def find(v)\uff1a while v.pi != v, go up the tree until you find the parent def union(u,v): make the root of the shorter subtree point to the root of the taller one, keeping the rank of the tree as small as possible <= log(vertices)","title":"Greedy"},{"location":"CS170/Greedy/#greedy-algorithm","text":"","title":"Greedy Algorithm"},{"location":"CS170/Greedy/#minimum-spanning-tree","text":"Given an undirected graph G, find the subset T of edges with the smallest total weight that connects all vertices. Fact : optimal solution must not have a cycle, because if a cycle exists, it is possible to remove an edge to maintain the connectivity and yet have a cheaper total weight Greedy Algorithm \uff1a repeat: pick the shortest path remaining that does not create a cycle until all is connected Tree is and undirected graph that is connected and has no cycle. (any vertex can be the root) Claim1 : Any 2 of these properties implies the third and implies that the graph is a tree 1. T is connected 2. T has no cycles 3. e = v - 1 Cut in Graph : is a partition of the vertices into some subsets and its complement, S and V - S, and possibly edges crossing it. Claim2 : lightest edge in any cut also appears in some MST. Claim3 : Suppose x has no edges connecting some cut, and we want to add an edge, suppose e is the lightest edge connecting the cuts, then X + e is a MST","title":"Minimum Spanning Tree"},{"location":"CS170/Greedy/#meta-algorithm","text":"repeat: pick a cut such that x does not cross the cut, add the edge e with smallest weight edge in cut and add it to x until n-1 edges are added Kruskal : //Naive\uff1a x = empty, sort all edges by their weight for all edges in increasing order of weight, if x + e has no cycle, add it in x. //cost = cost of DFS per edge = O(m * n) //1st optimization: x = empty, sort all edge by their weight for all vertices, make set (v). for all e = (u,v) increasing order, if find(u) != find(v) which finds their connected components, then we merge u and v's connected components //cost = O(m x logn) => can get to O(m x log*n) Union-find Implementation : for each v in V, pi(v) = a pointer of the parent of v in a tree defining a connected component rank(v) = height of the subtree rooted at V def makeset(v): v.pi = v, v.rank = 0 def find(v)\uff1a while v.pi != v, go up the tree until you find the parent def union(u,v): make the root of the shorter subtree point to the root of the taller one, keeping the rank of the tree as small as possible <= log(vertices)","title":"Meta-Algorithm\uff1a"},{"location":"Data100/Data100_review/","text":"Data100 Revision Notes Sampling Probability Sample: chance of each individual selected is specified. Simple Random Sample: uniformly at random without replacement Pandas Accessors : df[colname/slices], df.loc[rowname, colname], df.iloc[rowpos, colpos] Groupby : groupby a certain criteria to form sub dataframes, then aggregate based on the agg funcs like sum, count, mean, first/last, max.min, etc. Can be filtered by a certain function, and will elimnate a whole subdataframe if not met Pivot : break into different groups by rows and also column, summing up (or custom agg func), then putting the result into a new df. String operators : series.str.len(), series.str.lower()/upper(), ser.str.replace(pattern, replacement), series.str.contains(pattern), sereis.str.extract(pattern) Regex Quantifiers : * // 0 or more occurrences + // 1 or more occurrences ? // 0 or 1 occurrences {x,y} // inclusively between x and y copies Metacharacters : [] // a set of equivalent single characters [^d] // not d A | BCD // matches either A or BCD () // grouping (?:) //grouping then ungrouping ^ // start anchor of the string $ // end anchor of the string Predefined characters : . // matches any char not '\\n' \\w //matches a letter, digit or underscore \\s //matches a single whitespace \\d //decimal digit \\ //escapes a special character Useful functions : re.split{pattern, string} re.sub{pattern, replace, string} re.findall{pattern, string} // returns a list of all matches Data Cleaning EDA Porperties : * Granularity: are they summaries (coarse) or are they individual record (fine) * Scope: covering area of interest * Temporality: when was the data last updated * Faithfulness: how trustworthy? Missing Data & Defaults : * drop records with missing values, might induce bias * imputation: by averaging or hot deck (random value) Linear Regression Simple : \\bar{y} = \\hat{\\theta_0} + \\hat{\\theta_1}\\bar{x} Ordinary : \\bar{y} = \\hat{\\theta_0} + \\sum_{i=1}^p\\hat{\\theta_i}\\bar{x} L2 Loss : square of the difference, heavily penalize the outlier, would predict to the mean in constant model L1 Loss : take absolute value of the difference, would predict to the median in constant model Optimization : \\hat{\\theta} = (\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y} \\hat{\\theta}_1 = r\\frac{\\sigma_y}{\\sigma_x} \\hat{\\theta}_0 =\\bar{y} - \\hat{\\theta}_1\\bar{x} Residual Plot : a good residual plot should show no pattern, otherwise some bias might present, which is not ideal. the residuals are gauranteed to have a mean of 0 if it has an intercept term. Gradient Descent find gradient : g= \\frac{dL}{d\\theta}\\bigg|_{\\theta = \\theta_i} update theta : \\theta_{i+1} = \\theta_i - \\alpha*g Bias Variance Decomposition Expected Squared Loss (Observed) = Noise + Bias^2 + Variance E[(y-f_\\theta(x))^2] = E[(y-h_\\theta(x))^2] + (h(x) - E[f_\\theta(x)])^2 + E[(E[f_{\\hat{\\theta}}(x)] - f_{\\hat{\\theta}}(x))^2] increasing model complexity will increase variance and decrease the model bias. Regularization Normal Regression : \\hat{\\theta} = arg \\min_\\theta\\frac{1}{n}\\sum_{i=1}^nLoss(y_i, f_\\theta(x_i)) Adding Regularization : \\hat{\\theta} = arg \\min_\\theta\\frac{1}{n}\\sum_{i=1}^nLoss(y_i, f_\\theta(x_i)) + \\lambda*R(\\theta) Larger $\\lambda$ values implies more regularization weight and will lead to less complexity. LASSO Regularization : R(\\theta) = \\sum_{j=1}^d\\vert\\theta_j\\vert Ridge Regularization : R(\\theta) = \\sum_{j=1}^d\\theta_j^2 K-Fold Validation After the train-test split, we then separate the training set into k different sets, using the k-1 sets for training and then validate the accuracy with the set left, repeating this process for k times. SQL SELECT[columns] //must include in every query FROM [tables] //must include in every query WHERE [conditions] //filter rows GROUP BY [columns] HAVING [conditions] //filter groups ORDER BY [conditions] LIMIT [int] //show *int* rows Other keywords: LIKE [pattern] //just like regex, but simpler CASE [column] AS [type] AVG(column) // get the average of a column MAX(column) COUNT(column) Joins : * Inner joins: return columns that both tables have the record * Left outer joins: return all columns of the left and matching columns of the right * Right outer joins: return all columns of the right and matching columns of the left * Outer joins: return a column if either left or right has a match * Cross joins: return all pairs of columns PCA SVD : for an original matrix M with size $m \\times n$, we can decompose it into: * $\\mathbb{U}$ matrix ($m \\times m$): orthonormal matrix, gives the principle component by $PC = \\mathbb{U} \\mathbb{\\Sigma}$ * $\\Sigma$ matrix ($m \\times n$): diagonal matrix, descending order. Note that $\\sum\\sigma^2 / n$ gives the variance. * $\\mathbb{V}^T$ matrix ($n \\times n$): orthonormal matrix, gives the principle component by $PC = M \\mathbb{V}$ AlWAYS center the data before doing PCA. The principle components will tell us the direction of the maximum varaince. Logistic Regression Predicts over the probability of $P(Y=1 \\vert x).$ Notice that there is a linear relationship: ln\\bigg(\\frac{P(Y=1 \\vert x)}{P(Y=0 \\vert x)}\\bigg) Solving backward to get p: t =ln\\frac{p}{1-p} \\implies p = \\sigma(t) = \\frac{1}{1 + e^{-t}} where\\hspace{0.5cm}t = x^T\\theta Cross Entropy Loss : Loss = -y\\ln\\hat{y} - (1-y)\\ln(1-\\hat{y}) Thresholding : classify y to be 1 if the predicted probability is more than threshold, else classify to be 0. Classification Metircs : * True Positive: actual postive, predicted positive * True Negative: actual negative, predicted negative * False Positive: actual negative, predicted positive * False Negative: actual positive, preidcted negative Classification Evaluations : * Accuracy: how many are actually classified correctly? $TP + TN/n$ * Precision: out of all the positives predicted, how many are acutally positive? $TP/(TP+FP)$ * Recall (True positive rate): how many actual positives are predicted to be positive? $TP/(TP+FN)$ * False Positive rate: what proportion of actual negatives are predicted to be positive? $FP/(FP+TN)$ The ROC curve plots the tradeoff between precision vs recall when we change the threshold. Decision Tree weighted entropy : entropy of a single node: S = -\\sum_{c}pc * log_2pc weighted entropy is defined as entropy scaled by the fraction of sample in the node preventing tree growth : * Setting a maximum tree depth * Setting a minimum number of samples to split a node Random Forests : Building multiple decision trees and then vote. Clustering k-means : pick an arbitrary k, and then place k random points as cluster centers, then iterate for: 1. color points according to the closest center 2. move center to the mean of the colors Loss Function : Inertia : the sum of the squared distance between data points and the cluster center Distortion : weighted inertia based on the number of data points Agglomerative Clustering : Every data points start to be a unique cluster, then join close data points with one another until there are only k clusters remaining.","title":"Data 100 Revision"},{"location":"Data100/Data100_review/#data100-revision-notes","text":"","title":"Data100 Revision Notes"},{"location":"Data100/Data100_review/#sampling","text":"Probability Sample: chance of each individual selected is specified. Simple Random Sample: uniformly at random without replacement","title":"Sampling"},{"location":"Data100/Data100_review/#pandas","text":"Accessors : df[colname/slices], df.loc[rowname, colname], df.iloc[rowpos, colpos] Groupby : groupby a certain criteria to form sub dataframes, then aggregate based on the agg funcs like sum, count, mean, first/last, max.min, etc. Can be filtered by a certain function, and will elimnate a whole subdataframe if not met Pivot : break into different groups by rows and also column, summing up (or custom agg func), then putting the result into a new df. String operators : series.str.len(), series.str.lower()/upper(), ser.str.replace(pattern, replacement), series.str.contains(pattern), sereis.str.extract(pattern)","title":"Pandas"},{"location":"Data100/Data100_review/#regex","text":"Quantifiers : * // 0 or more occurrences + // 1 or more occurrences ? // 0 or 1 occurrences {x,y} // inclusively between x and y copies Metacharacters : [] // a set of equivalent single characters [^d] // not d A | BCD // matches either A or BCD () // grouping (?:) //grouping then ungrouping ^ // start anchor of the string $ // end anchor of the string Predefined characters : . // matches any char not '\\n' \\w //matches a letter, digit or underscore \\s //matches a single whitespace \\d //decimal digit \\ //escapes a special character Useful functions : re.split{pattern, string} re.sub{pattern, replace, string} re.findall{pattern, string} // returns a list of all matches","title":"Regex"},{"location":"Data100/Data100_review/#data-cleaning-eda","text":"Porperties : * Granularity: are they summaries (coarse) or are they individual record (fine) * Scope: covering area of interest * Temporality: when was the data last updated * Faithfulness: how trustworthy? Missing Data & Defaults : * drop records with missing values, might induce bias * imputation: by averaging or hot deck (random value)","title":"Data Cleaning EDA"},{"location":"Data100/Data100_review/#linear-regression","text":"Simple : \\bar{y} = \\hat{\\theta_0} + \\hat{\\theta_1}\\bar{x} Ordinary : \\bar{y} = \\hat{\\theta_0} + \\sum_{i=1}^p\\hat{\\theta_i}\\bar{x} L2 Loss : square of the difference, heavily penalize the outlier, would predict to the mean in constant model L1 Loss : take absolute value of the difference, would predict to the median in constant model Optimization : \\hat{\\theta} = (\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y} \\hat{\\theta}_1 = r\\frac{\\sigma_y}{\\sigma_x} \\hat{\\theta}_0 =\\bar{y} - \\hat{\\theta}_1\\bar{x} Residual Plot : a good residual plot should show no pattern, otherwise some bias might present, which is not ideal. the residuals are gauranteed to have a mean of 0 if it has an intercept term.","title":"Linear Regression"},{"location":"Data100/Data100_review/#gradient-descent","text":"find gradient : g= \\frac{dL}{d\\theta}\\bigg|_{\\theta = \\theta_i} update theta : \\theta_{i+1} = \\theta_i - \\alpha*g","title":"Gradient Descent"},{"location":"Data100/Data100_review/#bias-variance-decomposition","text":"Expected Squared Loss (Observed) = Noise + Bias^2 + Variance E[(y-f_\\theta(x))^2] = E[(y-h_\\theta(x))^2] + (h(x) - E[f_\\theta(x)])^2 + E[(E[f_{\\hat{\\theta}}(x)] - f_{\\hat{\\theta}}(x))^2] increasing model complexity will increase variance and decrease the model bias.","title":"Bias Variance Decomposition"},{"location":"Data100/Data100_review/#regularization","text":"Normal Regression : \\hat{\\theta} = arg \\min_\\theta\\frac{1}{n}\\sum_{i=1}^nLoss(y_i, f_\\theta(x_i)) Adding Regularization : \\hat{\\theta} = arg \\min_\\theta\\frac{1}{n}\\sum_{i=1}^nLoss(y_i, f_\\theta(x_i)) + \\lambda*R(\\theta) Larger $\\lambda$ values implies more regularization weight and will lead to less complexity. LASSO Regularization : R(\\theta) = \\sum_{j=1}^d\\vert\\theta_j\\vert Ridge Regularization : R(\\theta) = \\sum_{j=1}^d\\theta_j^2","title":"Regularization"},{"location":"Data100/Data100_review/#k-fold-validation","text":"After the train-test split, we then separate the training set into k different sets, using the k-1 sets for training and then validate the accuracy with the set left, repeating this process for k times.","title":"K-Fold Validation"},{"location":"Data100/Data100_review/#sql","text":"SELECT[columns] //must include in every query FROM [tables] //must include in every query WHERE [conditions] //filter rows GROUP BY [columns] HAVING [conditions] //filter groups ORDER BY [conditions] LIMIT [int] //show *int* rows Other keywords: LIKE [pattern] //just like regex, but simpler CASE [column] AS [type] AVG(column) // get the average of a column MAX(column) COUNT(column) Joins : * Inner joins: return columns that both tables have the record * Left outer joins: return all columns of the left and matching columns of the right * Right outer joins: return all columns of the right and matching columns of the left * Outer joins: return a column if either left or right has a match * Cross joins: return all pairs of columns","title":"SQL"},{"location":"Data100/Data100_review/#pca","text":"SVD : for an original matrix M with size $m \\times n$, we can decompose it into: * $\\mathbb{U}$ matrix ($m \\times m$): orthonormal matrix, gives the principle component by $PC = \\mathbb{U} \\mathbb{\\Sigma}$ * $\\Sigma$ matrix ($m \\times n$): diagonal matrix, descending order. Note that $\\sum\\sigma^2 / n$ gives the variance. * $\\mathbb{V}^T$ matrix ($n \\times n$): orthonormal matrix, gives the principle component by $PC = M \\mathbb{V}$ AlWAYS center the data before doing PCA. The principle components will tell us the direction of the maximum varaince.","title":"PCA"},{"location":"Data100/Data100_review/#logistic-regression","text":"Predicts over the probability of $P(Y=1 \\vert x).$ Notice that there is a linear relationship: ln\\bigg(\\frac{P(Y=1 \\vert x)}{P(Y=0 \\vert x)}\\bigg) Solving backward to get p: t =ln\\frac{p}{1-p} \\implies p = \\sigma(t) = \\frac{1}{1 + e^{-t}} where\\hspace{0.5cm}t = x^T\\theta Cross Entropy Loss : Loss = -y\\ln\\hat{y} - (1-y)\\ln(1-\\hat{y}) Thresholding : classify y to be 1 if the predicted probability is more than threshold, else classify to be 0. Classification Metircs : * True Positive: actual postive, predicted positive * True Negative: actual negative, predicted negative * False Positive: actual negative, predicted positive * False Negative: actual positive, preidcted negative Classification Evaluations : * Accuracy: how many are actually classified correctly? $TP + TN/n$ * Precision: out of all the positives predicted, how many are acutally positive? $TP/(TP+FP)$ * Recall (True positive rate): how many actual positives are predicted to be positive? $TP/(TP+FN)$ * False Positive rate: what proportion of actual negatives are predicted to be positive? $FP/(FP+TN)$ The ROC curve plots the tradeoff between precision vs recall when we change the threshold.","title":"Logistic Regression"},{"location":"Data100/Data100_review/#decision-tree","text":"weighted entropy : entropy of a single node: S = -\\sum_{c}pc * log_2pc weighted entropy is defined as entropy scaled by the fraction of sample in the node preventing tree growth : * Setting a maximum tree depth * Setting a minimum number of samples to split a node Random Forests : Building multiple decision trees and then vote.","title":"Decision Tree"},{"location":"Data100/Data100_review/#clustering","text":"k-means : pick an arbitrary k, and then place k random points as cluster centers, then iterate for: 1. color points according to the closest center 2. move center to the mean of the colors Loss Function : Inertia : the sum of the squared distance between data points and the cluster center Distortion : weighted inertia based on the number of data points Agglomerative Clustering : Every data points start to be a unique cluster, then join close data points with one another until there are only k clusters remaining.","title":"Clustering"},{"location":"EE120/Lec1/","text":"EE120 Lecture 1 Signals are function Discrete Time (DT) Signals: \\mathbb{Z} => \\mathbb{R} \\ or \\ \\mathbb{C} Continuous Time (CT) Sginals: \\mathbb{R} => \\mathbb{R} \\ or \\ \\mathbb{C} x represents a signal as a whole, and x(n) refers to a value in that signal, and in CT, use x(t) DT Impulse Kreneker Delta: \\delta(n) = \\begin{cases} 1&\\text{n=0}\\\\ 0&\\text{otherwise} \\end{cases} Claim: Any DT signal can be decomposed into a linear combination of shifted impulses DT Unit-Step u(n) = \\begin{cases} 0&\\text{n < 0}\\\\ 1& \\text{n >= 0}\\\\ \\end{cases} u(n) = \\sum_{k=0}^{\\infty}\\delta(n-k) let m = n - k, we can rewrite it as... u(n) = \\sum_{m=-\\infty}^{n}\\delta(m) This can be represented as capturing the delta 0 value (which is 1) and keep that for the rest of the journey to n. You can also write the impulse back as a linear combination of unit steps like: \\delta(n) = \\frac{u(n) - u(n-1)}{1} Adding the derivation of 1 reminds of the derivative ! Systems Systems are also functions. They have an input x and an output y, which are both scalar signals . This is thus a single input single output system :O Signal is the mapping that takes an x in the domain and maps it to the corresponding y, these xs and ys are NOT values but functions with its individual plots. if x= [\\mathbb{R} \\rArr \\mathbb{R}] \\ and \\ y= [\\mathbb{R} \\rArr \\mathbb{R}] , then F is a CT system. if x= [\\mathbb{Z} \\rArr \\mathbb{R}] \\ and \\ y= [\\mathbb{Z} \\rArr \\mathbb{R}] , then F is a DT system. We might also see sampling functions and other forms that transforms discrete into continuous. The Linearity of System Linearity is established if satisfying the following conditions: Scaling: x_1 \\rArr y_1 \\rArr \\alpha x_1 \\rArr \\alpha y_1 Additivity: x_1 \\rArr y_1 x_2 \\rArr y_2 \\rArr x_1 + x_2 \\rArr y_1 + x_2 Thus, Linearity means: \\rArr \\alpha x_1 + \\beta x_2 \\rArr \\alpha y_1 + \\beta x_2","title":"EE120 Lecture 1"},{"location":"EE120/Lec1/#ee120-lecture-1","text":"","title":"EE120 Lecture 1"},{"location":"EE120/Lec1/#signals-are-function","text":"Discrete Time (DT) Signals: \\mathbb{Z} => \\mathbb{R} \\ or \\ \\mathbb{C} Continuous Time (CT) Sginals: \\mathbb{R} => \\mathbb{R} \\ or \\ \\mathbb{C} x represents a signal as a whole, and x(n) refers to a value in that signal, and in CT, use x(t)","title":"Signals are function"},{"location":"EE120/Lec1/#dt-impulse","text":"Kreneker Delta: \\delta(n) = \\begin{cases} 1&\\text{n=0}\\\\ 0&\\text{otherwise} \\end{cases} Claim: Any DT signal can be decomposed into a linear combination of shifted impulses","title":"DT Impulse"},{"location":"EE120/Lec1/#dt-unit-step","text":"u(n) = \\begin{cases} 0&\\text{n < 0}\\\\ 1& \\text{n >= 0}\\\\ \\end{cases} u(n) = \\sum_{k=0}^{\\infty}\\delta(n-k) let m = n - k, we can rewrite it as... u(n) = \\sum_{m=-\\infty}^{n}\\delta(m) This can be represented as capturing the delta 0 value (which is 1) and keep that for the rest of the journey to n. You can also write the impulse back as a linear combination of unit steps like: \\delta(n) = \\frac{u(n) - u(n-1)}{1} Adding the derivation of 1 reminds of the derivative !","title":"DT Unit-Step"},{"location":"EE120/Lec1/#systems","text":"Systems are also functions. They have an input x and an output y, which are both scalar signals . This is thus a single input single output system :O Signal is the mapping that takes an x in the domain and maps it to the corresponding y, these xs and ys are NOT values but functions with its individual plots. if x= [\\mathbb{R} \\rArr \\mathbb{R}] \\ and \\ y= [\\mathbb{R} \\rArr \\mathbb{R}] , then F is a CT system. if x= [\\mathbb{Z} \\rArr \\mathbb{R}] \\ and \\ y= [\\mathbb{Z} \\rArr \\mathbb{R}] , then F is a DT system. We might also see sampling functions and other forms that transforms discrete into continuous.","title":"Systems"},{"location":"EE120/Lec1/#the-linearity-of-system","text":"Linearity is established if satisfying the following conditions: Scaling: x_1 \\rArr y_1 \\rArr \\alpha x_1 \\rArr \\alpha y_1 Additivity: x_1 \\rArr y_1 x_2 \\rArr y_2 \\rArr x_1 + x_2 \\rArr y_1 + x_2 Thus, Linearity means: \\rArr \\alpha x_1 + \\beta x_2 \\rArr \\alpha y_1 + \\beta x_2","title":"The Linearity of System"},{"location":"EE140/Lec1/","text":"EE140 Lecture 1","title":"EE140 Lecture 1"},{"location":"EE140/Lec1/#ee140-lecture-1","text":"","title":"EE140 Lecture 1"},{"location":"Musci27/Week2/","text":"Music as Collective Action Beat Terms Beat : a basic musician unit subdividing time. A regular, recurring pulse or background. The quicker beats succeed each other, the faster tempo . Meter : The effect of grouping accented and unaccented beats into a recurring pattern. Measure (Bar): A single occurence of this pattern beginning with one strong beat. accent (accented or unaccented): The stressing or emphasis of a beat, by making it louder and causing our sense of meter Metrical - there is a beat presense; Nonmetrical - no beat Meters Duple meter Two in a measure (bar): ONE (down) two (up) ONE two Four in a measure: ONE (down) two three four (up) ONE Triple meter Three in a measure: ONE (down) two three (up) Compound meter Two layers: ONE N/A N/A two N/A N/A / One two three four five six Meter vs Rhythm: rhythm is the flow of sound; meter is the patterns in the sounds. Syncopation : The displacement of accents in foreground rhythm against background meter Music as Way of Life Pitch Staying put: on one level Whole step, Half step, and chromatic scale W2 Potpouri isicathamiya : South African choral song. - Apartheid - call and response : one singer calls of a phrase, and the rest of the people will respond. a cappella : purely vocal, nothing instrumental The Middle Ages : the age of Colombus, 1492, plainchant or Georgia chant : one of the medieval modes, consists of unaccompanied, monophonic sounds. In Paradisum : monophonic, and a little bit of melismas (p49) Columba aspexit : more melismas (p50) organum : The very first NOTATED polyphonic music -> an a capella genre Alleluia. Diffusa est gratia: firstly monophonic chant, then starts organum. (p53) Melodic contours : Is it by step or by leap? Is it ascending or descending? The flow of how the music moves. melody : an organized series of pitches tune : simple, easily singabl, catchy melody recitation : chanted words around oreinting reciting tones * pitch : rate of sound vibration * tone : the physical sound of a specific or definite pitch * scale : the pool of pitches available for making music * diatonic scale : CDEFGAB * chromatic scale : C#D#F#G#A# * sequence : duplication of words at different pitch levels","title":"Week2"},{"location":"Musci27/Week2/#music-as-collective-action","text":"","title":"Music as Collective Action"},{"location":"Musci27/Week2/#beat-terms","text":"Beat : a basic musician unit subdividing time. A regular, recurring pulse or background. The quicker beats succeed each other, the faster tempo . Meter : The effect of grouping accented and unaccented beats into a recurring pattern. Measure (Bar): A single occurence of this pattern beginning with one strong beat. accent (accented or unaccented): The stressing or emphasis of a beat, by making it louder and causing our sense of meter Metrical - there is a beat presense; Nonmetrical - no beat","title":"Beat Terms"},{"location":"Musci27/Week2/#meters","text":"Duple meter Two in a measure (bar): ONE (down) two (up) ONE two Four in a measure: ONE (down) two three four (up) ONE Triple meter Three in a measure: ONE (down) two three (up) Compound meter Two layers: ONE N/A N/A two N/A N/A / One two three four five six Meter vs Rhythm: rhythm is the flow of sound; meter is the patterns in the sounds. Syncopation : The displacement of accents in foreground rhythm against background meter","title":"Meters"},{"location":"Musci27/Week2/#music-as-way-of-life","text":"","title":"Music as Way of Life"},{"location":"Musci27/Week2/#pitch","text":"Staying put: on one level Whole step, Half step, and chromatic scale","title":"Pitch"},{"location":"Musci27/Week2/#w2-potpouri","text":"isicathamiya : South African choral song. - Apartheid - call and response : one singer calls of a phrase, and the rest of the people will respond. a cappella : purely vocal, nothing instrumental The Middle Ages : the age of Colombus, 1492, plainchant or Georgia chant : one of the medieval modes, consists of unaccompanied, monophonic sounds. In Paradisum : monophonic, and a little bit of melismas (p49) Columba aspexit : more melismas (p50) organum : The very first NOTATED polyphonic music -> an a capella genre Alleluia. Diffusa est gratia: firstly monophonic chant, then starts organum. (p53) Melodic contours : Is it by step or by leap? Is it ascending or descending? The flow of how the music moves. melody : an organized series of pitches tune : simple, easily singabl, catchy melody recitation : chanted words around oreinting reciting tones * pitch : rate of sound vibration * tone : the physical sound of a specific or definite pitch * scale : the pool of pitches available for making music * diatonic scale : CDEFGAB * chromatic scale : C#D#F#G#A# * sequence : duplication of words at different pitch levels","title":"W2 Potpouri"},{"location":"Musci27/Week3/","text":"Music Making Worlds Harmony Harmony of the Spheres: 'Heaven and earth and all that which is done in them by a higher dispensation do not exist without the discipline of music... This world was established through through music and can be governed by it' - Cassiodorus Texture: Describing the \"weave\" or blend of sounds and melodic lines occurring simultaneously in a section of music. Sequence : a type of more elaborate plainchant in which succeessive phrases of a text receive near identical tratment. Monophonic : a musical texture where only a single melodic line is heard. Voices sing 'as if' with one voice. Polyphonic : describing a musical texture where two or more melodic lines are heard independently and simultaneously. They may be said to interact in counterpoint . Imitative Polyphony : singing the same motive and words, but at different pitch levels and enters at different time, generating a feeling of harmony. Homophonic : describing a texture with only one melody of real interest, sustaining a hierarchy that everything else supports, either 1).melody and accompaniment or 2). thythmic unison Dynamics Softest ---> Loudest strings/woodwinds/brass/percussion powerful vs noisy? W3 Potpouri drone : a single two-note chord running continuously, using to create a feeling of serene yet intense spirituality CADENCE : stopping places soprano - alto - tenor - bass, from higher to lower Tone color and timbre: sound genral quality pp - p - mp - mf - f -ff softest --------> loudest","title":"Week3"},{"location":"Musci27/Week3/#music-making-worlds","text":"","title":"Music Making Worlds"},{"location":"Musci27/Week3/#harmony","text":"Harmony of the Spheres: 'Heaven and earth and all that which is done in them by a higher dispensation do not exist without the discipline of music... This world was established through through music and can be governed by it' - Cassiodorus Texture: Describing the \"weave\" or blend of sounds and melodic lines occurring simultaneously in a section of music. Sequence : a type of more elaborate plainchant in which succeessive phrases of a text receive near identical tratment. Monophonic : a musical texture where only a single melodic line is heard. Voices sing 'as if' with one voice. Polyphonic : describing a musical texture where two or more melodic lines are heard independently and simultaneously. They may be said to interact in counterpoint . Imitative Polyphony : singing the same motive and words, but at different pitch levels and enters at different time, generating a feeling of harmony. Homophonic : describing a texture with only one melody of real interest, sustaining a hierarchy that everything else supports, either 1).melody and accompaniment or 2). thythmic unison","title":"Harmony"},{"location":"Musci27/Week3/#dynamics","text":"Softest ---> Loudest strings/woodwinds/brass/percussion powerful vs noisy?","title":"Dynamics"},{"location":"Musci27/Week3/#w3-potpouri","text":"drone : a single two-note chord running continuously, using to create a feeling of serene yet intense spirituality CADENCE : stopping places soprano - alto - tenor - bass, from higher to lower Tone color and timbre: sound genral quality pp - p - mp - mf - f -ff softest --------> loudest","title":"W3 Potpouri"},{"location":"Musci27/Week4/","text":"Music as Making You Expression emotion is personal, and affect is more social Dissonance (discord): pitches or chords that feel unstabe or sound to create tension resolution : letting a consonant chord to follow the dissonance to put it at rest. Consonance : chords that are at rest. Ostinato : repeated low pitch. Recitative : a technique for declaiming musical speeches Aria : An aria is a classical music form composed for a solo voice. basso continuo : the combination of the bass line and harmonies basso ostinato or ground bass : the background music repeating again and again, typically used by baroque musicians, and ostinato refers to short music gesture repeating itself Language Modulation : when a piece changes its key Madrigal : a highly poetic secular vocal composition originating in the italian renaissance,(1500s later) usually for three or more voices. Syllabic : One syllable matching to one note Mellismatic : One syllable being stretched to two or more notes W4 Potpouri harpsichord : another keyboard instrument with a busy, crisp sound Chanson : a french art music of the middle ages and renaissance tonality \" names we give to a system and its hierachies, the feeling of the centrality of one tone in the hierarchy mode : major or mino, quality provided by the scale system key : one of the 12 positions where the mode is built","title":"Week4"},{"location":"Musci27/Week4/#music-as-making-you","text":"","title":"Music as Making You"},{"location":"Musci27/Week4/#expression","text":"emotion is personal, and affect is more social Dissonance (discord): pitches or chords that feel unstabe or sound to create tension resolution : letting a consonant chord to follow the dissonance to put it at rest. Consonance : chords that are at rest. Ostinato : repeated low pitch. Recitative : a technique for declaiming musical speeches Aria : An aria is a classical music form composed for a solo voice. basso continuo : the combination of the bass line and harmonies basso ostinato or ground bass : the background music repeating again and again, typically used by baroque musicians, and ostinato refers to short music gesture repeating itself","title":"Expression"},{"location":"Musci27/Week4/#language","text":"Modulation : when a piece changes its key Madrigal : a highly poetic secular vocal composition originating in the italian renaissance,(1500s later) usually for three or more voices. Syllabic : One syllable matching to one note Mellismatic : One syllable being stretched to two or more notes","title":"Language"},{"location":"Musci27/Week4/#w4-potpouri","text":"harpsichord : another keyboard instrument with a busy, crisp sound Chanson : a french art music of the middle ages and renaissance tonality \" names we give to a system and its hierachies, the feeling of the centrality of one tone in the hierarchy mode : major or mino, quality provided by the scale system key : one of the 12 positions where the mode is built","title":"W4 Potpouri"},{"location":"Projects/151_CPU/","text":"Project Review: EECS151_ASIC_CPU This is a review of the Verilog CPU me and Yichuan Ding co-designed and developed in EECS151 ASIC lab. Skeleton Code Almost all module file is provided with a port skeleton, except for memloader.v, Cache.v, and RISC-V control.v. The top module, and a pretty extensive test are also predefined and can be used for our convenience. As a result, it is unfair to say we \"developed a CPU from scratch\". We have a crazily solid foundation abefore we start actually implementing anything. Design Decisions We were tasked to design a three-stage pipelined CPU. Many other folks in our lab decided to directly pursue a five-stage CPU in order to win that non-existing apple competition :/. Still, we decided to go for a simpler three-stage pipline because it will be a lot easier to debug and have way less hazards to consider. The diagram above is the pipeline diagram we ended up using for the final design. In the initial design, we have IF, D+E, and M+WB. However, we later found out the need to put the DMEM on the brink of two pipeline stages because it is a clocked component and doesn't support asynchronous read. So we ended up having to put IMEM and DMEM on the edge between two stages to save cycles and improve performance. A very cool part to brag about this design is the special PC register logic in the Instruction Fetching stage. We have designed it so that the mux chooses PC or the branch/jump target to feed into the IMEM and PC_D register, but the PC_reg updates its value based on either PC+4 or target+4. This way, we can (brilliantly) avoid any control hazards from branching or jump and thus avoid writing any logic for branch prediction. This design has the additional benefit of not needing to reset the PC_reg to target-4, so we kept the entire system intuitive and readable. ALU The first task is to design an ALU for this CPU. This is supposed to be a fairly easy task. It is mainly constituted of case and if statements. One thing we didn't debug was the SRA. Verilog only treats a shift right to be arithmetic if the number to be shifted is explicitly signed. Otherwise it will still be a logical shift, even if you used >>> instead of >> for the shift. (SRA will put in signed bits to the left, whereas SRL will only put a bunch of zeroes.) Datapath and Control Writing the datapath and the control logic is also relatively simple. All the components are explicitly drawn, and connections are out there as well. It is more of a labor than an intellectual work to just to code down the stream. However, because we firstly had no idea what is a CSR(Control Status Register) and decided to ignore it for now, we ran into the middle of nowhere for quite a while. Turns out the test programs utilizes the CSR to check for errors. After finishing up the CSR, it is just a matter of labor work again to fix up the rest of the code. Of course we make terrible coding mistakes along the way, so we must do test-oriented programming, finding and fixing them manually. We fixed them in the order of arithmetic operations => load word => the rest of load instructions => store instructions. This part of debugging only took about a weekend, so it is quite a breeze comparing to what's coming next. Cache Cache is where we suffered the most for sure. We were asked to design and implement a direct-mapped cache that is 2kB, with each cache line 512 bits. The maximum size of a DRAM available to us, though, its only 128 bits wide (64*128), so each of the read and write operations take 4 repetitive steps to complete. This requires us to create a counter within the cache to keep track of what is being loaded already and how many more are required. In other words, the interface between the cache and the main memory is itself another small FSM! This is an exciting observation, and we decided to create a separate module to bring this obseration to life. This leads us to the mem-loader module, the bridge between cache and main memory. This design turns out to be fruitful in a lot of ways. Firstly, it carries out both the read and write at the same time, leaving the cache to be a black box to only needing to send requests by RW=1 or 0. Secondly, it makes the system very flexible and compatible to changes in adding additional layers of cache, or possibly swapping the lowest level of cache, while not confusing the main memory with changed requests. Lastly, this design is way easier to debug because it shows a clearer flow of signal and requests from the cache to the main memory. When cache is firstly implemented, our previous passed tests turn out to crumble down, even those R-type instructions that should be independent of any cache changes. Turns out the instruction cache isn't behaving as espected, so we connected only the icache and connected the data cache to a golden model for debugging purposes. This lead us to easily sort out the bug and solve it. However, icache processes no write requests, so it is considerably easier. Afterward, we need to solve for dcache and write request related bugs. Turns out the data_write_enable signal should not be impacted by the stalls. Fixing it fixed all the asm bugs, which means our cache works in some small programs for now. Debugging Cache The benchmark test, which are larger tests specifically for testing cache accuracy and performance, took considerably longer to run (probably hours). We were stucked here for a long time, even taking a peek at the waveform became a difficult thing. Turns out the time out cycle was set to be 5*10^n, something super large, and we ran into an infinite loop. Fixing that, we started to debug for the write back part of the cache. Writing back is definitely the trickiest bug of the entire project. We primarily worked on a program in which 'Go Bears!' are capitialized into 'GO BEARS!'. This is actually quite fun because the waveform tool can transform series of bits into ASCII characters, and you can see all kinds of nonsenses being produced XD. Turns out the bug is, our Cache FSM is lacking a stage of reading the right thing from the cache before writing things back. This is why you can see things like GOAB popping around our waveforms. Fixing this, and some other minor bugs in other bmark tests, gave us the desired results of passing all the bmark tests. TODOs I am satisfied with what we did in this project. However, our synthesis, as the final step, was unsuccessful. Despite solving all the latches and other bugs showing up in the log, we couldn't get the synthesized design to pass tests. This is quite disappointing. If given more time, we are confident that we can get the desing working in the synthesis stage. But all in all, I've surely learnt a lot from this project.","title":"151 Verilog CPU"},{"location":"Projects/151_CPU/#project-review-eecs151_asic_cpu","text":"This is a review of the Verilog CPU me and Yichuan Ding co-designed and developed in EECS151 ASIC lab.","title":"Project Review: EECS151_ASIC_CPU"},{"location":"Projects/151_CPU/#skeleton-code","text":"Almost all module file is provided with a port skeleton, except for memloader.v, Cache.v, and RISC-V control.v. The top module, and a pretty extensive test are also predefined and can be used for our convenience. As a result, it is unfair to say we \"developed a CPU from scratch\". We have a crazily solid foundation abefore we start actually implementing anything.","title":"Skeleton Code"},{"location":"Projects/151_CPU/#design-decisions","text":"We were tasked to design a three-stage pipelined CPU. Many other folks in our lab decided to directly pursue a five-stage CPU in order to win that non-existing apple competition :/. Still, we decided to go for a simpler three-stage pipline because it will be a lot easier to debug and have way less hazards to consider. The diagram above is the pipeline diagram we ended up using for the final design. In the initial design, we have IF, D+E, and M+WB. However, we later found out the need to put the DMEM on the brink of two pipeline stages because it is a clocked component and doesn't support asynchronous read. So we ended up having to put IMEM and DMEM on the edge between two stages to save cycles and improve performance. A very cool part to brag about this design is the special PC register logic in the Instruction Fetching stage. We have designed it so that the mux chooses PC or the branch/jump target to feed into the IMEM and PC_D register, but the PC_reg updates its value based on either PC+4 or target+4. This way, we can (brilliantly) avoid any control hazards from branching or jump and thus avoid writing any logic for branch prediction. This design has the additional benefit of not needing to reset the PC_reg to target-4, so we kept the entire system intuitive and readable.","title":"Design Decisions"},{"location":"Projects/151_CPU/#alu","text":"The first task is to design an ALU for this CPU. This is supposed to be a fairly easy task. It is mainly constituted of case and if statements. One thing we didn't debug was the SRA. Verilog only treats a shift right to be arithmetic if the number to be shifted is explicitly signed. Otherwise it will still be a logical shift, even if you used >>> instead of >> for the shift. (SRA will put in signed bits to the left, whereas SRL will only put a bunch of zeroes.)","title":"ALU"},{"location":"Projects/151_CPU/#datapath-and-control","text":"Writing the datapath and the control logic is also relatively simple. All the components are explicitly drawn, and connections are out there as well. It is more of a labor than an intellectual work to just to code down the stream. However, because we firstly had no idea what is a CSR(Control Status Register) and decided to ignore it for now, we ran into the middle of nowhere for quite a while. Turns out the test programs utilizes the CSR to check for errors. After finishing up the CSR, it is just a matter of labor work again to fix up the rest of the code. Of course we make terrible coding mistakes along the way, so we must do test-oriented programming, finding and fixing them manually. We fixed them in the order of arithmetic operations => load word => the rest of load instructions => store instructions. This part of debugging only took about a weekend, so it is quite a breeze comparing to what's coming next.","title":"Datapath and Control"},{"location":"Projects/151_CPU/#cache","text":"Cache is where we suffered the most for sure. We were asked to design and implement a direct-mapped cache that is 2kB, with each cache line 512 bits. The maximum size of a DRAM available to us, though, its only 128 bits wide (64*128), so each of the read and write operations take 4 repetitive steps to complete. This requires us to create a counter within the cache to keep track of what is being loaded already and how many more are required. In other words, the interface between the cache and the main memory is itself another small FSM! This is an exciting observation, and we decided to create a separate module to bring this obseration to life. This leads us to the mem-loader module, the bridge between cache and main memory. This design turns out to be fruitful in a lot of ways. Firstly, it carries out both the read and write at the same time, leaving the cache to be a black box to only needing to send requests by RW=1 or 0. Secondly, it makes the system very flexible and compatible to changes in adding additional layers of cache, or possibly swapping the lowest level of cache, while not confusing the main memory with changed requests. Lastly, this design is way easier to debug because it shows a clearer flow of signal and requests from the cache to the main memory. When cache is firstly implemented, our previous passed tests turn out to crumble down, even those R-type instructions that should be independent of any cache changes. Turns out the instruction cache isn't behaving as espected, so we connected only the icache and connected the data cache to a golden model for debugging purposes. This lead us to easily sort out the bug and solve it. However, icache processes no write requests, so it is considerably easier. Afterward, we need to solve for dcache and write request related bugs. Turns out the data_write_enable signal should not be impacted by the stalls. Fixing it fixed all the asm bugs, which means our cache works in some small programs for now.","title":"Cache"},{"location":"Projects/151_CPU/#debugging-cache","text":"The benchmark test, which are larger tests specifically for testing cache accuracy and performance, took considerably longer to run (probably hours). We were stucked here for a long time, even taking a peek at the waveform became a difficult thing. Turns out the time out cycle was set to be 5*10^n, something super large, and we ran into an infinite loop. Fixing that, we started to debug for the write back part of the cache. Writing back is definitely the trickiest bug of the entire project. We primarily worked on a program in which 'Go Bears!' are capitialized into 'GO BEARS!'. This is actually quite fun because the waveform tool can transform series of bits into ASCII characters, and you can see all kinds of nonsenses being produced XD. Turns out the bug is, our Cache FSM is lacking a stage of reading the right thing from the cache before writing things back. This is why you can see things like GOAB popping around our waveforms. Fixing this, and some other minor bugs in other bmark tests, gave us the desired results of passing all the bmark tests.","title":"Debugging Cache"},{"location":"Projects/151_CPU/#todos","text":"I am satisfied with what we did in this project. However, our synthesis, as the final step, was unsuccessful. Despite solving all the latches and other bugs showing up in the log, we couldn't get the synthesized design to pass tests. This is quite disappointing. If given more time, we are confident that we can get the desing working in the synthesis stage. But all in all, I've surely learnt a lot from this project.","title":"TODOs"}]}